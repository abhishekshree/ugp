%!TEX root = ../main.tex

% \section{A Multi-Stage Allocation Process}
\section{Introductory Dynamic Programming (Understanding Bellman)}

\subsection{Introduction}
\subsubsection{Notation}

The standard Dynamic Programming problem desribed throughout will be in metric space $\mathbb{X}$.

Given, $\Phi: \mathbb{X} \times \mathbb{X} \to \R$ and $\Gamma: \mathbb{X} \rightrightarrows \R$

For a fixed $x^0$, find a sequence ${x_m} = {x^1, ..., x^m, ...}$ such that
$$
\max \Phi(x^0, x^1) + \sum_{i=1}^{\infty}\delta^i \Phi(x^i, x^{i+1}), \text{ where } \delta \in (0,1) \text{ and } x^1 \in \Gamma(x^0), x^{i+1} \in \Gamma(x^i), m = 1,2,3,...
$$

Here $\delta$ is called the discount factor.

\subsubsection{Functional Equations}

Considering a space of Real numbers $\R$, we can define a functional equation as follows:

$$
f(x) = \max_{0 \le y \le x} [g(y) + h(x-y) + f(ay+b(x-y))]
$$

where $a,b \in \R$ and $g,h$ are given functions.

This can be seen as an \textbf{insurance problem} where functions $g$ and $h$ represent the different places where and initial premium $x$ is invested. The function $f$ represents the value of the policy at the end of the period.

\vspace{5mm}
We can define, $R_1(x, y) = g(y) + h(x-y)$, initially the aim is to determine $\max_{0 \le y \le x} R_1(x, y)$. The value of $x$ and $y$ obtained from this maximization can be used to generate the next element of the sequence $x_m$.

i.e. $x_1 = ay + b(x-y) = y_1 + (x_1-y_1)$

Further we define $R_2(x, y, y_1) = g(y) + h(x-y) + g(y_1) + h(x_1-y_1)$ where $(y, y_1) \in [0, x] \times [0, x_1]$

\vspace{5mm}
This can be generalized to $R_m(x, y, y_1, ..., y_{m-1}) = g(y) + h(x-y) + g(y_1) + h(x_1-y_1) + g(y_2) + h(x_2-y_2) + ... + g(y_{m-1}) + h(x_{m-1} - y_{m-1})$

and the maximization problem becomes
\[
\max R_m(x, y, y_1, ..., y_{m-1}) \text{ such that }
\]
\[ 
\begin{array}{ll}
    ay+b(x-y) = x_1 & 0 \le y \le x \\
    ay_1+b(x_1-y_1) = x_2 & 0 \le y_1 \le x_1 \\
    \vdots & \vdots\\
    ay_{m-1}+b(x_{m-1}-y_{m-1}) = x_m & 0 \le y_{m-1} \le x_{m-1}
\end{array}
\]

Here, $f_N(x) = \max_{t, y_i} R_N(x, y_1, ..., y_N)$ and $f_N(x)$ can be interpreted as the maximum return obtained from an N-stage decision with initial premium $x$ and $x \ge 0$.
$$
\therefore f_N(x) = \max_{0 \le y \le x} [g(y) + h(x-y) + f_{N-1}(ay+b(x-y))]
$$
or 
$$
f(x) = \max_{0 \le y \le x} [g(y) + h(x-y) + f(ay+b(x-y))] \text{ for } N \rightarrow \infty
$$

\subsection{Existence and Uniqueness Theorems}
Under the following assumptions, the functional equation has a unique solution, which is continuous at $x=0$ and has a value 0 at $x=0$. Moreover the solution is continuous at $x \ge 0$.
\newpage
The assumptions are:
\begin{enumerate}
    \item $g$ and $h$ are continuous on $[0, \infty)$ and g(0) = h(0) = 0
    \item If $m(x) = \max_{0 \le y \le x} [\max\{|g(y)|, |h(y)|\}]$ and $c = \max \{a, b\}$, then $\bigsum_{n=0}^{\infty} m(c^nx) < \infty \forall x \ge 0$
\end{enumerate}

\begin{proof}
    We define a mapping $T(f, y) = g(y) + h(x-y) + f(ay+b(x-y))$ and $f_{N+1}(x) = \max_{0 \le y \le x} T(f_N, y)$, assuming $g$ and $h$ to be non-negative.

    The sequence $\{f_N(x)\}$ is an increasing sequence of functions such that $f_N \rightarrow f$

    Prooving that $T$ is bounded above would suffice.

    \textbf{Method 1:} We know that $x_1 \le cx$
    $$
        \begin{array}{c}
            \therefore ax_1 + b(x_1 - y) \le cx_1 \le c^2x \\
            \implies T(f_N, y) \le 2(m(x) + m(cx) + ... + m(c^Nx))
        \end{array}
    $$
    Which proves that $T$ is bounded above by a constant. 

    \textbf{Method 2:} 
    $$
    \begin{array}{l}
        f_{N+1}(x) = \mathlarger{\sup}_{ 0 \le y \le x} T(f_N, y) \\
        f(x) \ge \mathlarger{\sup}_{ 0 \le y \le x} T(f_N, y) \\
        f(x) \ge T(f_N, y) \forall y \in [0, x] \\
        f(x) \ge T(f, y) \text{ as } N \rightarrow \infty\\
        \therefore \boxed{f(x) = \mathlarger{\sup}_{ 0 \le y \le x} T(f, y)}
    \end{array}
    $$
    Also, 
    \begin{align*}
        T(f_N, y) &= g(y) + h(x-y) + f_N(ay+b(x-y)) \\
                    &\le g(y) + h(x-y) + f(ay+b(x-y)) \\
                    &= T(f, y)
    \end{align*}
    \begin{align*}
        f_{N+1}(x) &= \mathlarger{\sup}_{ 0 \le y \le x} T(f_N, y) \\
                    &\le \mathlarger{\sup}_{ 0 \le y \le x} T(f, y) \\
        f_{N+1}(x) &\le \mathlarger{\sup}_{ 0 \le y \le x} T(f, y)
    \end{align*}
    As $N \rightarrow \infty$, 
    $$
    \boxed{f(x) \le \mathlarger{\sup}_{ 0 \le y \le x} T(f, y)}
    $$
    $$
    \therefore f(x) = \mathlarger{\sup}_{ 0 \le y \le x} T(f, y) \text{ using the above two inequalities}
    $$
\end{proof}

\subsubsection{Existence and Uniqueness with g and h not necessarily non-negative}
We can write, 
\begin{align*}
    f_1(x) &= \max_{0 \le y \le x} [g(y) + h(x-y)] \\
            &= \max_{y \in [0, x]} [g(y) + h(x-y) + 0] \\
            &= \max_{y \in \Gamma(x)} [g(y) + h(x-y)] & \Gamma(x) = [0, x] \text { a set valued map}
\end{align*}

\begin{tcolorbox}
    \begin{center}
        \textbf{\underline{Digression}}
    \end{center}

    We will use a generalized form of Berge Maximum Theorem in the following proof, which is described here.

    \textbf{Berge Maximum Theorem (Generalized Form)}

    Let $\Theta$ and $\mathbb{X}$ be two metric spaces and $\Gamma: \Theta \rightrightarrows \mathbb{X}$ is a compact valued correspondence. Let $\phi: \mathbb{X} \times \Theta \rightarrow \mathbb{R}$ be a continuous function.

    Let $\sigma(\theta) = argmax\{\phi(x, \theta): x \in \Gamma(\theta)\}$ and let $\phi^*(\theta) = \mathlarger{\max}_x\{\phi(x, \theta): x \in \Gamma(\theta)\}$

    \vspace{3mm}
    Let $\Gamma: \Theta \rightrightarrows \mathbb{X}$ is continuous at some $\theta \in \Theta$. Then,
    \begin{enumerate}
        \item $\sigma: \Theta \rightrightarrows \mathbb{X}$ is compact-valued, upper semi-continuous and closed at $\theta$.
        \item $\phi^*: \Theta \rightarrow \R$ is continuous at $\theta$.
    \end{enumerate}

    Some definitions that follow from above are:
    \begin{enumerate}
        \item {
            \textbf{Upper semi-continuous Map:}
            Let $\Gamma: \mathbb{X} \rightrightarrows \mathbb{Y}$, then $\Gamma$ is upper semi-continuous at $x \in \mathbb{X}$ if for any $\{x^m\} \in \mathbb{X}$ and $\{y^m\} \in \mathbb{Y}$ with $x^m_0 \rightarrow x$ and $y^m \in \Gamma(x^m)$, the sequence $\{y^m\}$ has a convergent subsequence such that its limit is in $\Gamma(x)$.
        }
        \item {
            \textbf{Compact valued Map:}
            $\Gamma: \mathbb{X} \rightrightarrows \mathbb{Y}$ is a compact valued correspondence which is upper semi-continuous if for a compact subset $S \subset \mathbb{X}$, the set $\Gamma(S)$ is compact.
        }
        \item {
            \textbf{Closed Map:}
            $\Gamma: \mathbb{X} \rightrightarrows \mathbb{Y}$ is closed at $x \in \mathbb{X}$ if for any $\{x^m\} \in \mathbb{X}$ with $x^m \rightarrow x$ and $\{y^m\} \in \mathbb{Y}$ with $y^m \rightarrow y$ such that $y^m \in \Gamma(x^m)$ for each m, we have $y \in \Gamma(x)$. 
        }
    \end{enumerate}
\end{tcolorbox}

By applying the generalized form of Berge Maximum Theorem, we can show that $f_1(x)$ is continuous.

\vspace{5mm}
$T(f_N, y) = g(y) + h(x-y) + f_N(ay+b(x-y))$ and $f_{N+1}(x) = \mathlarger{\max}_{0 \le y \le x} T(f_N, y)$.

\begin{align*}
    f_{N+1}(x) = T(f_N, y_N) \ge T(f_N, y_N+1) \\
    f_{N+2}(x) = T(f_N+1, y_N+1) \ge T(f_N+1, y_N)
\end{align*}

Define, 
$$
    u_N(x) = \sup_{0 \le z \le x} |f_N(z) - f_N+1(z)|
$$

and
\begin{align*}
    f_{N+1}(x) - f_{N+2}(x) \ge T(f_N, y_{N+1}) - T(f_N+1, y_{N+1})\\
    f_{N+1}(x) - f_{N+2}(x) \le T(f_N, y_{N}) - T(f_N+1, y_{N})\\
\end{align*}
From these two inequalities, we get
$$
|f_{N+1}(x) - f_{N+2}(x)| \le \max \{|T(f_N, y_{N+1}) - T(f_N+1, y_{N+1})|, |T(f_N, y_{N}) - T(f_N+1, y_{N})|\}
$$
$$
|T(f_N, y_{N+1}) - T(f_N+1, y_{N+1})| = |f_N(ay_N +  b(x-y_N)) - f_{N+1}(ay_{N+1} +  b(x-y_{N+1})) \le u_N(cx)
$$
$$
|T(f_N, y_{N}) - T(f_N+1, y_{N})| = |f_N(ay_N +  b(x-y_N)) - f_{N+1}(ay_{N} +  b(x-y_{N})) \le u_N(cx)
$$
$\because ay+b(x-y) \le cx$ where $c = \max\{a, b\}$

$$
\therefore \boxed{u_{N+1}(x) \le u_N(cx)} \text{ for } N \in \N 
$$

We now need to estimate $u_1(x)$
\begin{align*}
    f_2(x) = T(f_1, y_1)\\
    f_1(x) - f_2(x) \ge g(y_1) + h(x-y_1) - T(f_1, y_1)\\
    f_2(x) - f_1(x) \le f_1(ay_1 +  b(x-y_1))
\end{align*}

Let $y_0$ be the maximizing point to obtain $f_1(x)$ i.e. $f_1(x) = g(y_0) + h(x-y_0)$ and $f_2(x) \ge T(f_1, y_0)$

\begin{align*}
    f_1(x) - f_2(x) &\ge g(y_0) + h(x-y_0) - T(f_1, y_0)\\
                    &= -f_1(ay_0 +  b(x-y_0))\\
    f_2(x) - f_1(x) &\ge f_1(ay_0 +  b(x-y_0))\\
\end{align*}
\[
    |f_2(x) - f_1(x)| \le \max\{f_1(ay_1 +  b(x-y_1)), |f_1(ay_0 +  b(x-y_0))|\} \implies
    |f_2(x) - f_1(x)| \le 2m(cx) \forall z \in [0, x]
\]

$$\therefore \boxed{u_1(x) \le 2m(cx)}$$

$$
u_{N+1}(x) \le u_N(cx) \le u_{N-1}(c^2x) \le ... \le u_1(c^Nx) \le 2m(c^{N+1}x)
$$
$$
\therefore {u_{N+1}(x) \le 2m(c^{N+1}x) \text{ and } u_1(x) \le 2m(cx)} \text{ for } N \in \N
$$
$$
\implies \boxed{u_N(x) \le 2m(c^{N}x) \text{ for } N \in \N}
$$

From the above result we can say that, 
$$
\bigsum_{N=1}^{\infty} u_N(x) \le \bigsum_{N=1}^{\infty} 2m(c^{N}x)
$$
that is 
$$
\bigsum_{N=1}^{\infty} u_N(x) \le \infty \text{(convergent)}
$$
$$
\implies u_N(x) \rightarrow 0 \text{ as } N \rightarrow \infty
$$
$\implies f_N(x)$ is a cauchy sequence (which then implies that $f_N-f_M$ can be expressed as a sum of differences).
$$
\therefore f(x) = \lim_{N \rightarrow \infty} f_N(x) \text{ exists and is continuous } \forall x \ge 0
$$

\vspace{5mm}

Now we know that, 
\begin{align*}
    f_{N+1}(x) &\ge T(f_N, y)\\
               &\ge g(y) + h(x-y) + f_N(ay+b(x-y)) \text{ for } y \in [0, x]\\
\end{align*}

For large $N$, 
\begin{align*}
    f(x) &\ge g(y) + h(x-y) + f(ay+b(x-y))\\
        &\ge \mathlarger{\max}_{0 \le y \le x} T(f, y) & (A)\\
\end{align*}

For a bounded sequence $\{y_N\}$ with $y_N \rightarrow y^*$ for some $y^* \in [0, x]$ and $N \rightarrow \infty$ we have,

\begin{align*}
    f(x) &= T(f, y^*) \\
        &\le \mathlarger{\max}_{0 \le y \le x} T(f, y) & (B)\\
\end{align*}

using (A) and (B), 
$$
\boxed{f(x) = \mathlarger{\max}_{0 \le y \le x} T(f, y)}
$$

\subsubsection{Uniqueness}
By proving the existence, we know that,
$$f(x) = \mathlarger{\max}_{0 \le y \le x} T(f, y)$$
Assume on the contrary that there are two such solutions $f \& F$
\begin{align*}
    f(x) = T(f, y) \ge T(f, w)\\
    F(x) = T(F, w) \ge T(F, y)\\
\end{align*}

Now,
\begin{align*}
    |f(x) - F(x)| &\le \max\{|T(f, y) - T(F, y)|,|T(f, w) - T(F, w)|\}\\
                    &\le \max\{|f(ay+b(x-y) - F(ay+b(x-y))|,|f(aw+b(x-w) - F(aw+b(x-w))|\}\\
\end{align*}

From previous definition, $u(x) = \mathlarger{\sup}_{0 \le z \le x} f(z) - F(z)$
We know that $u(x) \ge 0$ and $u(x) \rightarrow 0$ as $x \rightarrow \infty$.

\begin{align*}
    \therefore 0 \ge u(x) \ge |f(x) - F(x)| \\
    \implies |f(x) - F(x)| \rightarrow 0 \text{ as } x \rightarrow \infty\\
    \implies f(x) = F(x) \text{ for all } x \ge 0
\end{align*}

$\therefore$ The solution obtained is unique.

\subsection{Approximations}

\begin{theorem}
    Let $f_0(x)$ satisfy the following conditions:
    \begin{itemize}
        \item $f_0(x)$ is continuous for $x \ge 0$
        \item $f_0(0) = 0$
    \end{itemize}

    Then if the conditions of existence are fulfilled, the sequence defined by
    $$
        f_{n+1}(x) = \max_{0 \le y \le x} T(f_n, y) \text{ for } n \in \mathbb{W}
    $$
    converges to the solution $f(x)$ of the problem, uniformly in any finite interval.
\end{theorem}

\subsubsection{Approximation in the policy space}
We call a sequence of allocations, i.e. a sequence of admissible choices of $y$, a policy and a policy which yields $f(x)$ an optimal policy. 

The duality that exists in the theory of dynamic programmingarises
from the interconnection between the functions $f(x)$ which measure the maximum return and the policies which yield these maximum returns. Actually a policy is a function, since a policy is a determination of $y$ as a function of $x$. If the policy is not unique, $y$ will not be a single-valued function of $x$.

Just as we can approximate in the space of the functions $f(x)$, so we can approximate in the space of policies, $y(x)$.

\begin{theorem}
    Let $f_0(x)$ be the result of an initial approximation in the policy space, that is, $f_0(x)=T(f_0, y_0(x))$ where $y_0(x)$ is any continuous function of $x$ satisfying $0 \le y_0(x) \le x$, the sequence defined by
    $$
    f_{n+1}(x) = \max_{0 \le y \le x} T(f_n, y) \text{ for } n \in \mathbb{N}
    $$
    converges to the solution $f(x)$, uniformly and in a finite interval.
\end{theorem}
\begin{proof}
    By the definition of $f_n(x)$, we have
    $$f_{n+1} \ge f_n \ge f_0 \text{ }\forall \text{ }\mathbb{N}$$
    Proving that $f_0(x)$ is continuous, will imply continuity of $f_n(x)$.

    $$
    f_0(x) = g(y_0) + h(x-y_0) + \cdots\\
    $$
    where $g$ and $h$ are continuous functions.

    Hence, $f_0(x)$ which is iteratively obtained from $g(x)$ and $h(x)$ is also continuous in a finite interval.

\end{proof}
We can construct examples for such a solution, say
\begin{example*}
    $f(x) = T(f, y(x))$ be an iterative form of the solution.

    If the optimal policy consisted of the choice $y=0$ continually, the solution can be represented by the equation $f(x)=h(x)+f(bx)$, with the value of $b$ such that $h(b^nx) < \infty$
    
    i.e.,
    \begin{align*}
        f(x) &= h(x) + f(bx)\\
             &= h(x) + h(bx) + f(b^2x)\\
             &= h(x) + h(bx) + h(b^2x) + f(b^3x)\\
             &= \cdots\\
             &= \bigsum_{n=0}^{\infty} h(b^n x)\\
    \end{align*}

    Here, $h(b^n x) < \infty$ for all $n \in \mathbb{N}$, so the series converges and $f(x)$ is a continuous function of $x$.
\end{example*}

\subsection{Properties of the solution}
\subsubsection{Convexity}

\begin{theorem} If, along with existence assumptions, we impose
the conditions that $g$ and $h$ be convex functions of $x$, then $f(x)$ will be a
convex function, and for each value of $x$, $y$ will equal 0 or $x$.
\end{theorem}

\begin{proof}
    $g$ and $h$ are convex functions and, 
    $$f_1(x) = \max_{0 \le y \le x}(g(y) + h(x-y))$$
    In the interval $[0, x]$, $g$ and $h$ are convex, hence $g(y) + h(x-y)$ is also convex.

    $\because$ Maximum of a convex function must occur at the endpoints of the interval, i.e. $y=0$ or $y=x$. We can say that, 
    $$f_1(x) = \max(g(y), h(x))$$

    Then, 
    $$f_2(x) = \max_{0 \le y \le x}(g(y) + h(x-y) + f_1(ay + b(x-y)))$$
    similarly follows,
    $$f_2(x) = \max(g(x) + f_1(ax), h(x) + f_1(bx))$$

    Therefore we can inductively conclude that $f_n(x)$ is convex thus, the limit of the sequence $f(x)$ is also convex.
\end{proof}

\subsubsection{Concavity}
\begin{theorem}
    \textbf{[Weak Version]} If, along with existence assumptions, we impose the conditions that $g$ and $h$ be strictly concave functions of $x$, then $f(x)$ will be a strictly concave function, with an optimal policy which is unique.
\end{theorem}

\begin{lemma} If $G(x, y)$ is a concave function of $x$ and $y$ for $x, y \ge 0$, then $f(x)$ as defined by $f(x) = \mathlarger{\max}_{0 \le y \le x} G(x, y)$ is a concave function of $x$ for $x > 0$.
\end{lemma}

\begin{theorem}
    \textbf{[Strong Version]} Assuming that,
    \begin{enumerate}
        \item $g(x)$ and $h(x)$ are both strictly concave for $x > 0$, monotone increasing with continuous derivatives and that $g(0) = h(0) = 0$.
        \item $\mathlarger{\frac{g'(0)}{(1 - a)} > \frac{h'(0)}{(1 - b)}}, h'(0) > g'(\infty), b > a$.
    \end{enumerate}
    Then the optimal policy has the following form:
    \begin{enumerate}
        \item $y=x$ for $0<x<\bar{x}$, where $\bar{x}$ is the root of $h'(0)=g'(x)+(b-a)g'(ax)+(b-a)ag'(a^2x) +\cdots$
        \item $y=y(x)$ for $x > \bar{x}$ where $y(x)$ is a function satisfying the inequalities $0 < y(x) < x$, and $y(x)$ is the solution of $\boxed{g'(y)-h'(x-y)+(a-b)f'(ay+b(x-y))=0}$
    \end{enumerate}
\end{theorem}

\begin{tcolorbox}
    \begin{center}
        \textbf{\underline{A Result on Concavity}}
    \end{center}
    Let $g(x) = \mathlarger{\sup_{0\le y \le x}} \phi(x,y)$ and $\epsilon > 0$ $\exists y_1 \in [0, x]$ such that, 

    $$\phi(x_1, y_1) > g(x_1) - \epsilon$$
    $$\phi(x_2, y_2) > g(x_2) - \epsilon$$

    \begin{align*}
        \therefore \lambda(\phi(x_1, y_1)) + (1-\lambda)(\phi(x_2, y_2)) > \lambda g(x_1) + (1-\lambda)g(x_2) - \epsilon \\
        \implies \phi (\lambda (x_1, y_1) + (1-\lambda)(x_2, y_2)) > \lambda g(x_1) + (1-\lambda)g(x_2) - \epsilon \\
        \implies \phi (\lambda x_1 + (1-\lambda)x_2, \lambda y_1 + (1-\lambda)y_2) > \lambda g(x_1) + (1-\lambda)g(x_2) - \epsilon \\
    \end{align*}
    We can also say that, 
    \begin{align*}
        {\max_{0 \le y \le \lambda x_1 + (1-\lambda)x_2}} \phi (\lambda x_1 + (1-\lambda)x_2, y) \ge \lambda g(x_1) + (1-\lambda)g(x_2) - \epsilon \\
        \implies g(\lambda x_1 + (1-\lambda)x_2) \ge \lambda g(x_1) + (1-\lambda)g(x_2) - \epsilon \\
    \end{align*}
    \vspace{-1cm}
    $$\implies g \text{ is concave.}$$
\end{tcolorbox}

\begin{proof}
    It is known that, 
    $$f_1(x) = \max_{0 \le y \le x}(g(y) + h(x-y))$$

    $\because g'(0) > h'(0) \implies g'(y)-h'(x-y)>0$ for $y \in [0, x]$ with the roots at $y=x$ and in the interval $[0, x'], \forall x' > y$

    At $y=x$, the value $x^*$ for which $g'(x^*)-h'(0) = 0$ is the root of the equation $g'(x)-h'(0) = 0$.

    This equation has precisely one solution due to the fact that $g$ and $h$ are strictly concave.

    For $x \ge x^*$, let $y_1 (x)$ be the unique solution of $g' (y) = h' (x - y)$. Then, $f_1(x) = g(y_1) + h(x-y_1)$ and $f'(x) = [g'(y_1) - h'(x-y_1)]\frac{dy_1}{dx} + h'(x-y_1) = h'(x-y_1)$ for $x > x_1$.

    Further, 
    \begin{equation*}
        f_2(x) = 
        \begin{cases}
            g'(x), & \text{if } 0 \le x \le x_2\\
            h'(x-y_2) + bf_1(ay_2+b(x-y_2)), & \text{if } x > x_2
        \end{cases}
    \end{equation*}
    where $y_2(x)$ is the unique solution of $g'(y) = h'(x-y) + (b-a)f_1(ay+b(x-y))$
    Inductively, 

    $$
    f_{n+1}(x) = \mathlarger{\max}_{0 \le y \le x} \left\{ g(y) + h(x-y) + f_n(ay+b(x-y)) \right\}
    $$

    The solution space hence can be divided into three parts from, $[0, x_2]$, $[x_2, x_1]$ and $[x_1, \infty)$, wherein the inequality $f'_{n+1}(x) \ge f'_{n}(x)$ holds for all $x$.
    
    For $n=1$, 
    $$
    f'_2(x) = \frac{bg'(y_2) - ah'(x-y_2)}{b-a} \text{ for } x \ge x_2
    $$
    
    and 
    
    $$
    f'_1(x) = \frac{bg'(y_1) - ah'(x-y_1)}{b-a} \text{ for } x \ge x_1
    $$
    
    For $[x_1, \infty)$, and $f'$ is monotonically decreasing and $y_2 < y_1$, $f'_2(x) > f'_1(x)$. In, $[0, x_2]$, $f'_2(x) = f'_1(x)$.
    
    \vspace{5mm}
    In $[x_2, x_1]$,
    $$
    f'_1(x) = g'(x)
    $$
    $$
    f'_2(x) = \frac{bg'(y_2) - ah'(x-y_2)}{b-a} \text{ for } x \ge x_2
    $$

    Hence, in the interval, $0 \le y_2 \le x$ for $x \in [x_2, x_1]$,
    $$
        f'_2(x) \ge \frac{bg'(x) - ah'(0)}{b-a} > g'(x)
    $$
    And we assume that, $g'(x) \ge h'(0) \therefore f'_2(x) > f'_1(x)$

    Or, for $x_1 > x_2 > x_3 > \cdots > x_n > 0$, $f'_1(x) \le f'_2(x) \le f'_3(x) \le \cdots \le f'_{n}(x) \le \cdots$
    
    \vspace{5mm}
    Since $f_n (x)$ converges to $f(x)$, $f_n' (x)$ to $f' (x)$, $y_n (x)$ to $y (x)$ and $x_n$ to $\bar{x}$, the solution has the form, 

    $$
    f(x) = \mathlarger{\max}_{0 \le y \le x} \left\{ g(y) + h(x-y) + f(ay+b(x-y)) \right\}
    $$

    \begin{align*}
        \frac{df}{dy} = 0 
        \implies g'(y)-h'(x-y)+(a-b)f'(ay+b(x-y))=0
    \end{align*}
\end{proof}

% \textbf{A Problem:} Let $c, d > 0$ and $$
\subsection{Examples}

% \begin{example} Let $c,d>0$ and $0<b\le a<1$. Let $f(x)=\max[cy-y^2 + d(x-y) - (x-y)^2+ f(ay +b(x-y))]$, $f(0)=0$.
% Show that, in the maximum interval over which the g and h functions are both increasing, i.e. $0 < x < \min(c/2, d/2)$, $f(x)$ has the following form, which depends on the sign of $c/(1 - a) - d/(1 - b)$.

% \vspace{5mm}
% \textbf{Solution:} Let $\alpha = c/(1-a)$ and $\beta = d/(1-b)$. Then, $\alpha$ and $\beta$ can have these relationships.

% \textbf{I $\rightarrow$} $\alpha = \beta$
% \end{example}

\begin{example} Show that, The continuous solution of $f(x) = \max[cx^d + f(ax), ex^g + f(bx)], f(0) =0$, subject to 
\begin{enumerate}
    \item $a, b \in (0, 1)$, $c, d, e, g >0$,
    \item $0 < d<g$
\end{enumerate}
is given by, 
    \begin{equation*}
        f(x) = 
        \begin{cases}
            \frac{cx^d}{1-a^d}, & \text{if } 0 \le x \le \bar{x}\\
            ex^g+f(bx), & \text{if } x \ge \bar{x}
        \end{cases}
    \end{equation*}
where, $\bar{x} = \mathlarger{\left(\frac{c/(1-a^d)}{e/(1-b^d)}\right)^{\frac{1}{g-d}}}$.

\vspace{5mm}
\textbf{Solution:} Assuming, $A$ as the choice $cx^d + f(ax)$ and $B$ as the choice $ex^g + f(bx)$, we have any solution of the form $A^{k_1^t}B^{k_2^t}$ or the number of times A and B are chosen to reach a solution at the $t^{th}$ step.

At optimal time, represented by say $\infty$, any choice of A or B would not change the solution. Mathematically, 

$$BA^\infty = A^\infty$$

Now, 

\begin{align*}
    A^\infty &= cx^d + f(ax) = cx^d + c(ax)^d + f(a^2x) + f(a^3x) + \cdots \\
             &= \frac{cx^d}{(1-a^d)}
\end{align*}

Similarly, $BA^{\infty} \implies f(x) = ex^g + \frac{cb^dx^d}{(1-a^d)}$.

Equating them we get, 
$$
ex^g + \frac{cb^dx^d}{(1-a^d)} = \frac{cx^d}{(1-a^d)} \implies x = {\left(\frac{c/(1-a^d)}{e/(1-b^d)}\right)^{\frac{1}{g-d}}}
$$

The value of $x$ is the optimal x, $\bar{x}$.

\textbf{Case I:} When $0 \le x \le \bar{x}$ We have to show that $\frac{cx^d}{1-a^d}$ is the solution. At $t=\infty$,
$$
\max\left[\frac{cx^d}{(1-a^d)}, ex^g + \frac{cb^dx^d}{(1-a^d)}\right] = \frac{cx^d}{(1-a^d)}
$$

This is true for small $x$ when $g>d>0$ and $b \in (0, 1)$. If we proceed for more values of x, the smallest value at which solution B becomes optimal is attained at $BA^\infty=A^\infty$ which occurs at $x = \bar{x}$ as calculated above.

\vspace{4mm}
\textbf{Case II:} When $x \ge \bar{x}$, we have to show that $ex^g + f(bx)$ is the solution. At $t=\infty$,

Define, $f_{AB} (x) = cx^d + ea^gx^g + f(abx)$ and $f_{BA} (x) = ex^g + cb^dx^d + f(abx)$. These are compositions of the two choices $A$ and $B$. It can be interpreted as the dominant choice at each step, as $f_{AB}$ will signify the choice of $A$ at the first step and $B$ at the second step and so on. Similarly, $f_{BA}$ will signify the choice of $B$ at the first step and $A$ at the second step and so on.

Now, their point of intersection occurs at $p = {\left(\frac{c/(1-b^d)}{e/(1-a^g)}\right)^{\frac{1}{g-d}}}$.

Now, $p < \bar{x}$ as $g > d$. At this point, $f_{AB} (x) < f_{BA} (x)$ for $x > p$ for $x > \bar{x}$. This arises a contradiction. 

$\therefore$ $f(x) = ex^g + f(bx)$ is the optimal choice only when $x \ge \bar{x}$.
\end{example}

\vspace{0.8cm}
\begin{example} Let us define the function $f_N(a) = \max_R [x_1 x_2 \cdots x_n] $ where $R$ is the region determined by the conditions
    \begin{enumerate}
        \item $x_1 + x_2 + \cdots + x_n = a, a > 0$
        \item $x_i \ge 0$
    \end{enumerate}
Prove that $f_N(a)$ satisfies the recurrence relation $f_N(a) = \mathlarger{\max_{0 \le x \le a}} xf_{N-1}(a-x), N \ge 2$ with $f_1(a) = a$.

\vspace{5mm}
\textbf{Solution:} We can solve this using mathematical induction.

{$N=1$:} $f_1(a) = \max[x_1] = a$ as $x_1 = a$. Hence the relation holds.

{Let $N=k$} be true. That is, $f_k(a) = \mathlarger{\max_{0 \le x \le a}} xf_{k-1}(a-x)$.

{For $N=k+1$}, 
\begin{align*}
    f_{k+1}(x) &= \max[x_1 x_2 \cdots x_{k+1}] \\
               &= \max[\max[x_1 x_2 \cdots x_k] x_{k+1}] \\
               &= \max[x_{k+1}f_k(a-x_{k+1})] & \text{ as } x_1 + x_2+ \cdots + x_k = a-x_{k+1}
\end{align*}

$\because$ The relationship hold for all $N \in \mathbb{N}$. The recurrence relation holds.

This completes the proof.
\end{example}

\begin{example} For the previous $f$, show inductively that $f_N(a) = \frac{a^N}{N^N}$, and hence establish the arithmetic-geometric mean inequality, for $x_i > 0$.

\vspace{5mm}
\textbf{Solution:} Given, $f_N(a) = \mathlarger{\max_{0 \le x \le a}} xf_{N-1}(a-x)$.

For $N=1$ $f_1(a) = a = \frac{a^1}{1^1}$ which is true.

Let $f_k(a) = \frac{a^k}{k^k}$ hold as well.

For, $N=k+1$, $f_{k+1}(a) = \mathlarger{\max_{0 \le x \le a}} xf_{k}(a-x) = \mathlarger{\max_{0 \le x \le a}} x\frac{(a-x)^k}{k^k}$ which is maximized at $x=\frac{a}{k+1}$ resulting in 

$f_{k+1}(x) = \frac{a^{k+1}}{(k+1)^{k+1}}$

$\because$ The relationship hold for all $N \in \mathbb{N}$. The recurrence relation holds.

This completes the proof.
\end{example}

\begin{example}
Define a function,
$$
f_N(a) = \min_{R} \bigsum_{i=1}^N x_i^p, p>0
$$

where $R$ is a region defined by, 
\begin{enumerate}
    \item $\bigsum_{i=1}^N x_i \ge a, a > 0$
    \item $x_i \ge 0$
\end{enumerate}

Show that $f_N(a)$ satisfies the recurrence relation
$$
f_N(a) = \min_{0\le x \le a} [x^p + f_{N-1}(a-x)], N \ge 2 \text{ with } f_1(a) = a^p
$$

\vspace{5mm}
\textbf{Solution:} We can solve this inductively as, 

For $N=1$, $f_1(a) = a^p$, which is true.

Let for $N=k$, $f_k(a) = \mathlarger{\min_{0\le x \le a}} [x^p + f_{k-1}(a-x)]$

For, $N=k+1$,

\begin{align*}
    f_{k+1}(a) &= \min_{R} \bigsum_{i=1}^{k+1} x_i^p \\
               &= \min_{R} [\min_{R} \bigsum_{i=1}^{k} x_i^p + x_{k+1}^p] \\
               &= \min_{0\le x \le a} [x_{k+1}^p + f_{k}(a-x)]
\end{align*}

$\because$ The relationship hold for all $N \in \mathbb{N}$. The recurrence relation holds.

This completes the proof.
\end{example}

\begin{example}
Let $f(x)$ and $F(x)$ be the continuous solutions of the above equations under the assumptions that $u(x, y)$ and $v (x,y)$ are continuous in $x$ and $y \forall x,y >0$, with $0 <a,b <1$, and that $\bigsum_{n=0}^\infty m(c^nz) < \infty$ where 
$m(z) = \mathlarger{\max_{0\le x \le z} [\max_{0 \le y \le x} \max {| u(x, y) |, | v(x, y) |}]}$.
If $\mathlarger{\max_{0\le x \le z} {\max_{0\le y \le x } |u(x,y)-v(x,y)|}=D(z)}$ and $\bigsum_{n=0}^\infty D(c^nz) < \infty$, $c=\max (a, b)$, show that,
$$
|f(x)-F(x)| < \bigsum_{n=0}^\infty D(c^nz)
$$

\vspace{5mm}
\textbf{Solution:} We define, 
\begin{align*}
    f_1(x) = \max_{0\le y \le x} u(x, y) \\
    f_{N+1}(x) = \max_{0\le y \le x} \max \big\{ u(x, y), f_N(ay+b(x-y)) \big\} \\
    F_1(x) = \max_{0\le y \le x} v(x, y) \\
    F_{N+1}(x) = \max_{0\le y \le x} \max \big\{ v(x, y), F_N(ay+b(x-y)) \big\} \\
\end{align*}

Also, $\mathlarger{\lim_{N \to \infty}} f_N(x) = f(x)$ and $\mathlarger{\lim_{N \to \infty}} F_N(x) = F(x)$.

Now, 
$$
|f_1(x) - F_1(x)| \le \mathlarger{\max_{0\le y \le x}} |u(x, y) - v(x, y)| \le D(x)
$$

Similarly,

\begin{align*}
    |f_{N+1}(x) - F_{N+1}(x)| &\le \mathlarger{\max_{0\le y \le x}} \max \big\{ |u(x, y) - v(x, y)|, |f_N(ay+b(x-y)) - F_N(ay+b(x-y))| \big\} \\
    &\le D(x) + \mathlarger{\max_{0\le y \le x}} |f_N(ay+b(x-y)) - F_N(ay+b(x-y))|
\end{align*}
\vspace{-5mm}
$\therefore$
\vspace{-5mm}
$$
|f_{N+1}(x) - F_{N+1}(x)| \le \bigsum_{n=0}^\infty D(c^nx)
$$

When, $N \to \infty$
\vspace{-5mm}
$$
|f(x) - F(x)| \le \bigsum_{n=0}^\infty D(c^nx)
$$

This completes the proof.
\end{example}
